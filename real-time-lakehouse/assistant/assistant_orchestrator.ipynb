{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f5c31bb",
   "metadata": {},
   "source": [
    "# LLM Data Assistant Orchestrator (Notebook UI)\n",
    "Ask questions like: “Why did conversion drop in the last 10 minutes?”\n",
    "This notebook retrieves context (Vector Search), prompts an LLM (Llama via Model Serving), guards SQL, executes against UC (Gold/Silver), renders charts, and logs interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24be47c",
   "metadata": {},
   "source": [
    "## 1) Setup: packages and env\n",
    "Installs/loads required packages, reads PATs and endpoints from env/.env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install databricks-sql-connector requests python-dotenv pandas matplotlib plotly\n",
    "import os, sys, subprocess, json, time, re\n",
    "\n",
    "# Ensure required packages\n",
    "def ensure(pkgs):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs])\n",
    "    except Exception as e:\n",
    "        print(\"pip install failed:\", e)\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception:\n",
    "    ensure([\"pandas\"]) ; import pandas as pd\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except Exception:\n",
    "    ensure([\"matplotlib\"]) ; import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import plotly.express as px\n",
    "except Exception:\n",
    "    ensure([\"plotly\"]) ; import plotly.express as px\n",
    "\n",
    "try:\n",
    "    from databricks import sql as dbsql  # used later\n",
    "except Exception:\n",
    "    ensure([\"databricks-sql-connector\"]) ; from databricks import sql as dbsql\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except Exception:\n",
    "    ensure([\"requests\"]) ; import requests\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "except Exception:\n",
    "    ensure([\"python-dotenv\"]) ; from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.expanduser(\"/Users/kritan/data-monorepo/.env\"))\n",
    "\n",
    "# Databricks SQL creds (PAT)\n",
    "DATABRICKS_HOST = os.getenv(\"DATABRICKS_HOST\")\n",
    "DATABRICKS_HTTP_PATH = os.getenv(\"DATABRICKS_HTTP_PATH\")\n",
    "DATABRICKS_TOKEN = os.getenv(\"DATABRICKS_TOKEN\")\n",
    "assert DATABRICKS_HOST and DATABRICKS_HTTP_PATH and DATABRICKS_TOKEN, \"Missing DBSQL env vars\"\n",
    "\n",
    "# Model Serving (Llama) endpoint\n",
    "MODEL_ENDPOINT_URL = os.getenv(\"MODEL_ENDPOINT_URL\")  # e.g., https://.../serving-endpoints/llama/invocations\n",
    "MODEL_TOKEN = os.getenv(\"MODEL_TOKEN\", DATABRICKS_TOKEN)\n",
    "assert MODEL_ENDPOINT_URL and MODEL_TOKEN, \"Missing model serving endpoint/token\"\n",
    "\n",
    "# Vector Search endpoint/index\n",
    "VS_ENDPOINT = os.getenv(\"VS_ENDPOINT\")\n",
    "VS_INDEX = os.getenv(\"VS_INDEX\")  # e.g., demo.ecommerce_rt.kb_docs_index\n",
    "assert VS_ENDPOINT and VS_INDEX, \"Missing Vector Search endpoint/index\"\n",
    "\n",
    "CATALOG = os.getenv(\"CATALOG\", \"demo\")\n",
    "SCHEMA = os.getenv(\"SCHEMA\", \"ecommerce_rt\")\n",
    "DB = f\"{CATALOG}.{SCHEMA}\"\n",
    "\n",
    "GOLD = f\"{DB}.gold_kpis\"\n",
    "SILVER = f\"{DB}.silver_events\"\n",
    "V_TIMESERIES = f\"{DB}.v_kpi_timeseries\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cfbfdd",
   "metadata": {},
   "source": [
    "## 2) Databricks SQL connection and query helpers\n",
    "Creates a connection and helpers to run SQL with guardrails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks import sql as dbsql\n",
    "\n",
    "def open_conn():\n",
    "    return dbsql.connect(\n",
    "        server_hostname=DATABRICKS_HOST,\n",
    "        http_path=DATABRICKS_HTTP_PATH,\n",
    "        access_token=DATABRICKS_TOKEN,\n",
    "    )\n",
    "\n",
    "ALLOWED_SCHEMAS = {DB}\n",
    "RAW_LIMIT = 200\n",
    "DEFAULT_TIME_WINDOW_MIN = 60*24  # 24h fallback\n",
    "\n",
    "DDL_DML_PATTERN = re.compile(r\"\\b(INSERT|UPDATE|DELETE|DROP|ALTER|CREATE|TRUNCATE|MERGE|GRANT|REVOKE)\\b\", re.I)\n",
    "MULTI_STMT_PATTERN = re.compile(r\";\\s*\\S\")\n",
    "\n",
    "TIME_FILTER_PATTERN = re.compile(r\"\\b(window_start|ts)\\b\", re.I)\n",
    "\n",
    "def enforce_whitelist(sql_text: str) -> str:\n",
    "    if DDL_DML_PATTERN.search(sql_text):\n",
    "        raise ValueError(\"Only SELECT allowed\")\n",
    "    if MULTI_STMT_PATTERN.search(sql_text):\n",
    "        raise ValueError(\"Multiple statements not allowed\")\n",
    "    # Force catalog/schema prefix\n",
    "    for token in re.findall(r\"\\bfrom\\s+([\\w\\.]+)|\\bjoin\\s+([\\w\\.]+)\", sql_text, re.I):\n",
    "        tbl = next((t for t in token if t), None)\n",
    "        if not tbl:\n",
    "            continue\n",
    "        parts = tbl.split('.')\n",
    "        if len(parts) == 1:\n",
    "            # inject default DB\n",
    "            sql_text = re.sub(fr\"\\b{tbl}\\b\", f\"{DB}.{tbl}\", sql_text)\n",
    "        elif len(parts) == 2:\n",
    "            # ensure catalog present\n",
    "            if parts[0] != CATALOG:\n",
    "                sql_text = sql_text.replace(tbl, f\"{CATALOG}.{tbl}\")\n",
    "    return sql_text\n",
    "\n",
    "def ensure_limits(sql_text: str) -> str:\n",
    "    if re.search(r\"\\bgroup\\s+by\\b\", sql_text, re.I):\n",
    "        return sql_text\n",
    "    if re.search(r\"\\blimit\\b\", sql_text, re.I):\n",
    "        return sql_text\n",
    "    return sql_text.rstrip() + f\"\\nLIMIT {RAW_LIMIT}\"\n",
    "\n",
    "def ensure_time_filter(sql_text: str) -> str:\n",
    "    if TIME_FILTER_PATTERN.search(sql_text) and re.search(r\"now\\s*\\(\\)\\s*-\\s*INTERVAL\", sql_text, re.I):\n",
    "        return sql_text\n",
    "    # Prefer window_start when present\n",
    "    col = \"window_start\" if \"window_start\" in sql_text else \"ts\"\n",
    "    if re.search(r\"\\bwhere\\b\", sql_text, re.I):\n",
    "        return re.sub(r\"\\bwhere\\b\", f\"WHERE {col} >= now() - INTERVAL {DEFAULT_TIME_WINDOW_MIN} minutes AND \", sql_text, flags=re.I)\n",
    "    return sql_text + f\"\\nWHERE {col} >= now() - INTERVAL {DEFAULT_TIME_WINDOW_MIN} minutes\"\n",
    "\n",
    "def run_sql(sql_text: str) -> pd.DataFrame:\n",
    "    sql_text = enforce_whitelist(sql_text)\n",
    "    sql_text = ensure_time_filter(sql_text)\n",
    "    sql_text = ensure_limits(sql_text)\n",
    "    with open_conn() as conn:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"USE CATALOG \" + CATALOG)\n",
    "        cur.execute(\"USE SCHEMA \" + SCHEMA)\n",
    "        cur.execute(\"EXPLAIN \" + sql_text)\n",
    "        cur.execute(sql_text)\n",
    "        cols = [d[0] for d in cur.description]\n",
    "        rows = cur.fetchall()\n",
    "    return pd.DataFrame(rows, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9371ea",
   "metadata": {},
   "source": [
    "## 3) Vector Search retrieval\n",
    "Fetch top-k knowledge snippets to ground the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd033bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def vs_query(query: str, k: int = 6, kinds=(\"table\",\"kpi\",\"rule\",\"example\")):\n",
    "    url = f\"{VS_ENDPOINT}/indexes/{VS_INDEX}/query\"\n",
    "    headers = {\"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "    body = {\n",
    "        \"query\": query,\n",
    "        \"k\": k,\n",
    "        \"filters\": {\"kind\": list(kinds)}\n",
    "    }\n",
    "    r = requests.post(url, headers=headers, json=body, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"results\", [])\n",
    "\n",
    "def summarize_context(results):\n",
    "    chunks = []\n",
    "    for r in results:\n",
    "        meta = r.get(\"metadata\", {})\n",
    "        title = meta.get(\"title\") or meta.get(\"table_name\") or \"doc\"\n",
    "        body = r.get(\"text\") or r.get(\"body\") or \"\"\n",
    "        chunks.append(f\"[{title}]\\n{body}\")\n",
    "    return \"\\n\\n\".join(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c15bc6",
   "metadata": {},
   "source": [
    "## 4) Prompting and LLM call\n",
    "Build system/user prompts, call Model Serving, and parse JSON plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0230930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(messages):\n",
    "    headers = {\"Authorization\": f\"Bearer {MODEL_TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "    body = {\"messages\": messages, \"max_tokens\": 512, \"temperature\": 0.1}\n",
    "    r = requests.post(MODEL_ENDPOINT_URL, headers=headers, json=body, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    # Adapt to endpoint response shape\n",
    "    content = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\") or data\n",
    "    return content\n",
    "\n",
    "SYSTEM = (\n",
    "\"\"\"You are a Databricks SQL expert assistant.\n",
    "- Only generate ANSI SQL SELECT statements against Unity Catalog objects in the provided schemas.\n",
    "- Prefer gold KPIs table: {gold} and views like {vts}.\n",
    "- Add LIMIT for row outputs; avoid CROSS JOINs unless necessary.\n",
    "- Output strictly a compact JSON: {\"sql\":\"...\",\"explanation\":\"...\",\"chart\":{\"type\":\"line\",\"x\":\"window_start\",\"y\":[\"gmv\"]}}\n",
    "\"\"\".format(gold=GOLD, vts=V_TIMESERIES)\n",
    ")\n",
    "\n",
    "def build_prompt(user_question: str, context_text: str):\n",
    "    return f\"Context:\\n{context_text}\\n\\nQuestion: {user_question}\\nReturn JSON only.\"\n",
    "\n",
    "import json\n",
    "\n",
    "def plan_query(user_question: str):\n",
    "    ctx = summarize_context(vs_query(user_question))\n",
    "    prompt = build_prompt(user_question, ctx)\n",
    "    content = call_llm([\n",
    "        {\"role\":\"system\",\"content\": SYSTEM},\n",
    "        {\"role\":\"user\",\"content\": prompt}\n",
    "    ])\n",
    "    try:\n",
    "        plan = json.loads(content)\n",
    "    except Exception:\n",
    "        # try to extract JSON block\n",
    "        m = re.search(r\"\\{[\\s\\S]*\\}\", content)\n",
    "        if not m:\n",
    "            raise ValueError(\"LLM did not return JSON\")\n",
    "        plan = json.loads(m.group(0))\n",
    "    # sanity defaults\n",
    "    plan.setdefault(\"chart\", {\"type\":\"line\",\"x\":\"window_start\",\"y\":[\"gmv\"]})\n",
    "    plan.setdefault(\"explanation\", \"\")\n",
    "    return plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853d72a8",
   "metadata": {},
   "source": [
    "## 5) Guardrails and execution\n",
    "Apply whitelist/limits/time filters, EXPLAIN, repair once on error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f16e37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_plan(plan: dict) -> pd.DataFrame:\n",
    "    sql_text = plan.get(\"sql\", \"\").strip()\n",
    "    if not sql_text.lower().startswith(\"select\"):\n",
    "        raise ValueError(\"Generated SQL must be SELECT\")\n",
    "    try:\n",
    "        df = run_sql(sql_text)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        err = str(e)\n",
    "        # one-shot repair with error info\n",
    "        ctx = summarize_context(vs_query(plan.get(\"explanation\", \"\") or \"\"))\n",
    "        repair_prompt = f\"Context:\\n{ctx}\\n\\nSQL had error: {err}\\nOriginal SQL: {sql_text}\\nFix and return JSON only with updated sql.\"\n",
    "        content = call_llm([\n",
    "            {\"role\":\"system\",\"content\": SYSTEM},\n",
    "            {\"role\":\"user\",\"content\": repair_prompt}\n",
    "        ])\n",
    "        try:\n",
    "            fixed = json.loads(content)\n",
    "            return run_sql(fixed.get(\"sql\", sql_text))\n",
    "        except Exception:\n",
    "            m = re.search(r\"\\{[\\s\\S]*\\}\", content)\n",
    "            if not m:\n",
    "                raise\n",
    "            fixed = json.loads(m.group(0))\n",
    "            return run_sql(fixed.get(\"sql\", sql_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4719e9",
   "metadata": {},
   "source": [
    "## 6) Interaction logging (Delta)\n",
    "Log question, context, SQL, timings; scrub email-like strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fe3968",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_TABLE = f\"{DB}.assistant_logs\"\n",
    "\n",
    "CREATE_LOG_SQL = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {LOG_TABLE} (\n",
    "  event_time TIMESTAMP,\n",
    "  user_question STRING,\n",
    "  plan_json STRING,\n",
    "  sql_text STRING,\n",
    "  row_count INT,\n",
    "  latency_ms BIGINT,\n",
    "  error STRING\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "\n",
    "EMAIL_RE = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
    "\n",
    "def scrub_pii(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    return EMAIL_RE.sub(\"<email>\", text)\n",
    "\n",
    "def log_interaction(question: str, plan: dict, sql_text: str, row_count: int, latency_ms: int, error: str = None):\n",
    "    with open_conn() as conn:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"USE CATALOG \" + CATALOG)\n",
    "        cur.execute(\"USE SCHEMA \" + SCHEMA)\n",
    "        cur.execute(CREATE_LOG_SQL)\n",
    "        cur.execute(\n",
    "            f\"INSERT INTO {LOG_TABLE} VALUES (current_timestamp(), ?, ?, ?, ?, ?, ?)\",\n",
    "            (\n",
    "                scrub_pii(question),\n",
    "                json.dumps(plan)[:900000],\n",
    "                scrub_pii(sql_text)[:900000],\n",
    "                int(row_count),\n",
    "                int(latency_ms),\n",
    "                scrub_pii(error) if error else None,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc0352",
   "metadata": {},
   "source": [
    "## 7) Notebook Chat UI\n",
    "Enter a question, run the assistant, render results and chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5045fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question, then plan → execute → plot → log\n",
    "question = \"Why did conversion drop in the last 10 minutes?\"\n",
    "minutes_override = 10\n",
    "\n",
    "print(\"Question:\", question)\n",
    "\n",
    "try:\n",
    "    t0 = time.time()\n",
    "    plan = plan_query(question)\n",
    "    df = execute_plan(plan)\n",
    "    latency_ms = int((time.time() - t0) * 1000)\n",
    "    print(\"Latency:\", latency_ms, \"ms\")\n",
    "\n",
    "    # Basic rendering\n",
    "    if not df.empty:\n",
    "        print(\"Rows:\", len(df))\n",
    "        # Choose plotting backend\n",
    "        if set([\"window_start\",\"gmv\"]).issubset(df.columns):\n",
    "            fig = px.line(df, x=\"window_start\", y=[c for c in [\"gmv\",\"orders\",\"active_users\",\"conversion_rate\"] if c in df.columns])\n",
    "            fig.update_layout(height=400, width=900)\n",
    "            fig.show()\n",
    "        else:\n",
    "            display_cols = df.columns[:6]\n",
    "            print(df[display_cols].head(10))\n",
    "    else:\n",
    "        print(\"No results.\")\n",
    "\n",
    "    # Log\n",
    "    try:\n",
    "        sql_text = plan.get(\"sql\", \"\")\n",
    "        log_interaction(question, plan, sql_text, int(len(df)), latency_ms)\n",
    "    except Exception as e:\n",
    "        print(\"Log error:\", e)\n",
    "except Exception as e:\n",
    "    print(\"Assistant error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32c29df",
   "metadata": {},
   "source": [
    "## 8) Example Questions & Testing\n",
    "Sample queries for demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_questions = [\n",
    "    \"What was GMV over the last 15 minutes?\",\n",
    "    \"Show orders per minute for the last hour\",\n",
    "    \"Which time window had the highest conversion rate today?\", \n",
    "    \"Break down active users by 5-minute windows in the last 30 minutes\",\n",
    "    \"Show me 20 sample purchases from the last 10 minutes\",\n",
    "    \"Compare GMV vs conversion rate trends over the last 2 hours\"\n",
    "]\n",
    "\n",
    "# Quick test multiple questions\n",
    "for i, q in enumerate(example_questions[:3]):\n",
    "    print(f\"\\n[{i+1}] {q}\")\n",
    "    try:\n",
    "        plan = plan_query(q)\n",
    "        print(\"SQL:\", plan.get(\"sql\", \"\")[:120], \"...\")\n",
    "        print(\"Explanation:\", plan.get(\"explanation\", \"\")[:80], \"...\")\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc033bba",
   "metadata": {},
   "source": [
    "## 9) Logs & Quality Metrics\n",
    "View recent interactions and performance stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4875c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check recent logs\n",
    "try:\n",
    "    logs_df = run_sql(f\"\"\"\n",
    "        SELECT event_time, user_question, sql_text, row_count, latency_ms, error\n",
    "        FROM {LOG_TABLE}\n",
    "        ORDER BY event_time DESC\n",
    "        LIMIT 20\n",
    "    \"\"\")\n",
    "    if not logs_df.empty:\n",
    "        print(\"Recent interactions:\")\n",
    "        print(logs_df[['event_time', 'user_question', 'row_count', 'latency_ms', 'error']].head(10))\n",
    "        \n",
    "        # Quick metrics\n",
    "        print(\"\\nMetrics:\")\n",
    "        print(f\"Total interactions: {len(logs_df)}\")\n",
    "        print(f\"Avg latency: {logs_df['latency_ms'].mean():.0f}ms\")\n",
    "        print(f\"P95 latency: {logs_df['latency_ms'].quantile(0.95):.0f}ms\")\n",
    "        print(f\"Error rate: {(logs_df['error'].notna().sum() / len(logs_df) * 100):.1f}%\")\n",
    "    else:\n",
    "        print(\"No logs found. Run some questions first.\")\n",
    "except Exception as e:\n",
    "    print(\"Log table not ready:\", e)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
